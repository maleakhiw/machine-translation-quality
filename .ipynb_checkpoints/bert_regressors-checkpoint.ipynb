{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_x5uBre17plC"
   },
   "source": [
    "# BERT Word Embeddings with Regressors\n",
    "\n",
    "**Author**: Harry Coppock, Faidon Mitzalis, Maleakhi Wijaya  \n",
    "**Date**: 20 February 2020\n",
    "\n",
    "The file contains the following items:\n",
    "- BERT Pre-processing and embedding algorithms\n",
    "- Regressors\n",
    "  - Feed Forward Neural Network\n",
    "  - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bP4KZyMM8CFS"
   },
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RPkMurb8m9a"
   },
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "YKzsa57loe9e",
    "outputId": "4374e59b-0841-4ec1-a6a4-d8b24497fe92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.17.5)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.11.15)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.4.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.14.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch-pretrained-bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "!pip install pytorch-pretrained-bert\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertAdam\n",
    "import logging\n",
    "import torch\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Shwt0Nk9rsWk"
   },
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPPPFTTrrpxe"
   },
   "outputs": [],
   "source": [
    "if not exists('enzh_data.zip'):\n",
    "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
    "    !unzip enzh_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "Xrru-NWy8_vL",
    "outputId": "71f33667-a030-4a89-b6b6-2d4b4a631650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EN-ZH---\n",
      "\n",
      "Source:  The last conquistador then rides on with his sword drawn.\n",
      "\n",
      "Translation:  最后的征服者骑着他的剑继续前进.\n",
      "\n",
      "Score:  -1.5284005772625449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# English-Chinese\n",
    "# Checking Data\n",
    "print(\"---EN-ZH---\")\n",
    "print()\n",
    "\n",
    "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
    "  print(\"Source: \",enzh_src.readline())\n",
    "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
    "  print(\"Translation: \",enzh_mt.readline())\n",
    "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
    "  print(\"Score: \",enzh_scores.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ikVaTfw6Fw59"
   },
   "outputs": [],
   "source": [
    "# Read scores (train)\n",
    "with open(\"./train.enzh.scores\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "scores_train = [float(string.replace(\"\\n\", \"\")) for string in content]\n",
    "\n",
    "# Read scores (dev)\n",
    "with open(\"./dev.enzh.scores\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "scores_dev = [float(string.replace(\"\\n\", \"\")) for string in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0lCJrkoGFWwH"
   },
   "outputs": [],
   "source": [
    "# Read english (train)\n",
    "with open(\"./train.enzh.src\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "english_train = content\n",
    "\n",
    "# Read english (dev)\n",
    "with open(\"./dev.enzh.src\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "english_dev = content\n",
    "\n",
    "# Read english (test)\n",
    "with open(\"./test.enzh.src\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "english_test = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sny4oI2pGX35"
   },
   "outputs": [],
   "source": [
    "# Read Chinese (train)\n",
    "with open(\"./train.enzh.mt\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "zh_train = content\n",
    "\n",
    "# Read Chinese (dev)\n",
    "with open(\"./dev.enzh.mt\") as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "zh_dev = content\n",
    "\n",
    "# Read Chinese (test)\n",
    "with open(\"./test.enzh.mt\") as f:\n",
    "    content = f.readlines()\n",
    "  \n",
    "zh_test = content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-padlBDfK9us"
   },
   "source": [
    "### BERT Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UbxZPhAvG6O7",
    "outputId": "70270a8d-f229-420d-fa58-2e05a1c19cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 48\n"
     ]
    }
   ],
   "source": [
    "# Calculating the maximum sentence length for the english and the german corpus\n",
    "# before tokenization\n",
    "max_sent_length_english = max([len(sentence.split()) for sentence in english_train])\n",
    "max_sent_length_zh = max([len(sentence.split()) for sentence in zh_train])\n",
    "max_sent_length = max(max_sent_length_english, max_sent_length_zh)\n",
    "print(\"Maximum sentence length:\", max_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "XLsgZPz_I3xX",
    "outputId": "c7200b7c-347f-44ac-e9b4-e5e6ddb1f119"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is a cased model but you have not set `do_lower_case` to False. We are setting `do_lower_case=False` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "# Initialise multilingual bert model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DExHKmNDqpGm"
   },
   "outputs": [],
   "source": [
    "def prep_corpus(corpus_en, corpus_zh, max_len):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "          - corpus: list of sentences\n",
    "          - max_len: maximum sentence length\n",
    "    Returns:\n",
    "          - tokenised ids tensors (#sentences, #max_sentence_length)\n",
    "          - segment flag for BERT (#sentences, #max_sentence_length)\n",
    "    \"\"\"\n",
    "    indexed_corpus = []\n",
    "    ids_corpus = []\n",
    "    \n",
    "    for counter, sentence in enumerate(corpus_en):\n",
    "        # Mark beginning and end of sentence    \n",
    "        marked_sentence_en = \"[CLS] \" + sentence + \" [SEP]\" \n",
    "        marked_sentence_zh = corpus_zh[counter]   \n",
    "        tokenized_sentence_en = tokenizer.tokenize(marked_sentence_en)    \n",
    "        tokenized_sentence_zh = tokenizer.tokenize(marked_sentence_zh)    \n",
    "        tokenized_sentence = tokenized_sentence_en + tokenized_sentence_zh\n",
    "\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "        # Add PADDING (id=0) to achieve fixed sentence length\n",
    "        padding_num = (max_len*2 - np.shape(indexed_tokens)[0])\n",
    "        indexed_tokens.extend([0]*padding_num)\n",
    "        # Append tokenized sentence to corpus\n",
    "        indexed_corpus.append(indexed_tokens)\n",
    "        # Set BERT setting\n",
    "        segments_ids = ([0]*len(tokenized_sentence_en) + [1]*len(tokenized_sentence_zh)\n",
    "        + [0]*padding_num )\n",
    "        ids_corpus.append(segments_ids)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor(indexed_corpus)\n",
    "    segments_tensors = torch.tensor(ids_corpus)\n",
    "  \n",
    "    return tokens_tensor, segments_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "_Y0bm8h94eWd",
    "outputId": "30c28390-4006-4f94-dd13-8b75018d5066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101, 10117, 12469, 25735, 11849, 11059, 48543, 10107, 10135, 10169,\n",
      "        10226, 79400, 34788,   119,   102,  4458,  2775,  5718,  3763,  4463,\n",
      "         6457,  8575,  5778,  2196,  5718,  2570,  6352,  6356,  2568,  7701,\n",
      "          119,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "# Get BERT token ids example\n",
    "emb = prep_corpus(english_train, zh_train, 75)\n",
    "print(emb[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-ChZebH8dz_"
   },
   "source": [
    "## Regressors\n",
    "\n",
    "This sections contains the Feed Forward Neural Network used to get the best result and commented LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hm7j3p9-Bqg9"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xxLiOizB8kLr",
    "outputId": "977981f7-ca0b-4754-8359-c5f09cf1ff8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.4.0, CUDA: 10.1\n"
     ]
    }
   ],
   "source": [
    "# Torch Setup\n",
    "print('Torch version: {}, CUDA: {}'.format(torch.__version__, torch.version.cuda))\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if not torch.cuda.is_available():\n",
    "  print('WARNING: You may want to change the runtime to GPU!')\n",
    "  DEVICE = 'cpu'\n",
    "else:\n",
    "  DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILJFhBd1zHtI"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\" Set all seeds to make results reproducible (deterministic mode).\n",
    "        When seed is a false-y value or not supplied, disables deterministic mode. \"\"\"\n",
    "\n",
    "    if seed:\n",
    "        logging.info(f\"Running in deterministic mode with seed {seed}\")\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "    else:\n",
    "        logging.info(f\"Running in non-deterministic mode\")\n",
    "\n",
    "set_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnNSwomBB5d9"
   },
   "source": [
    "### Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mn_bnwA_A_zE"
   },
   "outputs": [],
   "source": [
    "class EvaluationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Main model class for the project. (FFNN based)\n",
    "    \"\"\"\n",
    "    def __init__(self):   \n",
    "        super(EvaluationModel, self).__init__()      \n",
    "    \n",
    "        # Embedding layer (BERT) which will be freezed later \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "        \n",
    "        ########################################################################\n",
    "        # For LSTM, run this code and replace self.fc1 below, the rest is same\n",
    "        # self.lstm = nn.LSTM(768, 200, num_layers=1, bidirectional=True)\n",
    "        # self.fc1 = nn.Linear(200, 200) \n",
    "        ########################################################################\n",
    "\n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(768, 200) \n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.fc3 = nn.Linear(100, 50)\n",
    "        self.fc4 = nn.Linear(50, 1)  \n",
    "\n",
    "        # Create the loss, don't sum or average, we'll take care of it\n",
    "        # in the training loop for logging purposes\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, tokens_tensor, segments_tensors):\n",
    "        \"\"\"\n",
    "        Responsible for forward pass.\n",
    "        \"\"\"\n",
    "        out, _ = self.bert_layer(tokens_tensor, segments_tensors)\n",
    "        x = (out[-1])[:,0,:] # gets the last layer of embeddings\n",
    "        \n",
    "        ########################################################################\n",
    "        # For LSTM, run this code\n",
    "        # x = out[-1]\n",
    "        # x = x.permute(1, 0, 2)\n",
    "        # x,(h_n, c_n) = self.lstm(x)\n",
    "        # x = h_n[-1]\n",
    "        ########################################################################\n",
    "\n",
    "        # Skip this line for rnn, the rest is the same\n",
    "        x = x.view(tokens_tensor.size(0), -1)\n",
    "\n",
    "        # Pass to linear layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "    \n",
    "        return x\n",
    "  \n",
    "    def train_model(self, optim, train_tokens, train_segments, train_scores,\n",
    "                  val_tokens, val_segments, val_scores, test_tokens, test_segments,\n",
    "                  n_epochs=100, batch_size=64, shuffle=False):\n",
    "        \"\"\"Trains the model.\"\"\"\n",
    "        # Get batches for the training data\n",
    "        tokens_batches, segments_batches, labels_batches = \\\n",
    "                self.get_batches(train_tokens,\n",
    "                                 train_segments,\n",
    "                                  train_scores, batch_size)\n",
    "\n",
    "        for eidx in range(1, n_epochs + 1):\n",
    "            start_time = time.time()\n",
    "            epoch_loss = 0\n",
    "            epoch_items = 0\n",
    "\n",
    "            # Enable training mode\n",
    "            self.train()\n",
    "\n",
    "            # Shuffle the batch order or not\n",
    "            if shuffle:\n",
    "                batch_order = torch.randperm(tokens_batches.size(0))\n",
    "            else:\n",
    "                batch_order = torch.arange(tokens_batches.size(0))\n",
    "\n",
    "            # Start training\n",
    "            for iter_count, idx in enumerate(batch_order):\n",
    "                tokens_batch = tokens_batches[idx].to(DEVICE)\n",
    "                segments_batch = segments_batches[idx].to(DEVICE)\n",
    "                labels_batch = labels_batches[idx].to(DEVICE)\n",
    "        \n",
    "                # Clear the gradients\n",
    "                optim.zero_grad()\n",
    "\n",
    "                # Get output vector of size (batch size, 1)\n",
    "                x = self.forward(tokens_batch, segments_batch)\n",
    "\n",
    "                # Apply loss function\n",
    "                loss = self.loss(x.view(-1), labels_batch.view(-1))\n",
    "\n",
    "                # Backprop the average loss and update parameters\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                # Sum the loss for reporting, along with the denominator\n",
    "                batch_loss = loss.detach().mean()\n",
    "\n",
    "                time_spent = time.time() - start_time\n",
    "\n",
    "            # Evaluate on valid set every epoch\n",
    "            rmse, pearson = self.evaluate(val_tokens, val_segments, val_scores, batch_size=batch_size)\n",
    "            print(f'[Epoch {eidx:<3}] ended with valid rmse: {rmse:6.2f}, pearson: {pearson[0]:6.3f}')\n",
    "\n",
    "            # Write prediction result every epoch\n",
    "            self.test_model(test_tokens, test_segments, batch_size=10, epoch=eidx)\n",
    "\n",
    "    def evaluate(self, test_tokens, test_segments, test_scores, batch_size=32):\n",
    "        \"\"\"Evaluates given data set in evaluation mode.\"\"\"\n",
    "    \n",
    "        # Initialise results tensor\n",
    "        out = torch.tensor([])\n",
    "    \n",
    "        # Split tokens into batches\n",
    "        tokens_batches, segments_batches, labels_batches = \\\n",
    "                self.get_batches(test_tokens, test_segments,\n",
    "                                 test_scores, batch_size)\n",
    "        # Eval mode\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_order = torch.arange(tokens_batches.size(0))\n",
    "\n",
    "            # Start training\n",
    "            for iter_count, idx in enumerate(batch_order):\n",
    "\n",
    "                tokens_batch = tokens_batches[idx].to(DEVICE)\n",
    "                segments_batch = segments_batches[idx].to(DEVICE)\n",
    "        \n",
    "                # Get results from network\n",
    "                results = (self.forward(tokens_batch, segments_batch)).cpu()\n",
    "                temp = torch.cat((out,results),0)\n",
    "                out = temp\n",
    "\n",
    "        # Normalize by the number of tokens in the test set\n",
    "        RMSE = mean_squared_error(labels_batches.view(-1), out.view(-1), squared=False)\n",
    "        pears = pearsonr(labels_batches.view(-1), out.view(-1))\n",
    "    \n",
    "        # Switch back to training mode\n",
    "        self.train()\n",
    "\n",
    "        # return the perplexity and loss\n",
    "        return RMSE, pears\n",
    "\n",
    "    def test_model(self, test_tokens, test_segments, batch_size=10, epoch=0):\n",
    "        \"\"\"Test returns a txt file of the predicted test scores.\"\"\"\n",
    "    \n",
    "        # Initialise results tensor\n",
    "        out = torch.tensor([])\n",
    "    \n",
    "        # Split tokens into batches\n",
    "        tokens_batches, segments_batches = \\\n",
    "                self.get_batches(test_tokens, test_segments,\n",
    "                                None, batch_size, test=True)\n",
    "        # Eval mode\n",
    "        self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_order = torch.arange(tokens_batches.size(0))\n",
    "\n",
    "      \n",
    "            for iter_count, idx in enumerate(batch_order):\n",
    "                tokens_batch = tokens_batches[idx].to(DEVICE)\n",
    "                segments_batch = segments_batches[idx].to(DEVICE)\n",
    "        \n",
    "                # Get results from network\n",
    "                results = (self.forward(tokens_batch, segments_batch)).cpu()\n",
    "                temp = torch.cat((out,results), 0)\n",
    "                out = temp\n",
    "\n",
    "        # Write scores to txt file for prediction\n",
    "        path = \"/content/drive/My Drive/Colab Notebooks/NLP_group/en-zh/predictions\"\\\n",
    "              + str(epoch) + \".txt\"\n",
    "        np.savetxt(path,out.numpy())\n",
    "    \n",
    "        # Switch back to training mode\n",
    "        self.train()\n",
    "\n",
    "    def get_batches(self, tokens_tensor, segments_tensor,\n",
    "              labels, batch_size=64, test=False):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            - tokens_tensor: (number of sentence pairs, max_sentece_length*2)\n",
    "            - segments_tensor: (number of sentence pairs, max_sentece_length*2)\n",
    "            - labels: (number of sentence pairs, max_sentece_length*2)    \n",
    "        Returns:\n",
    "            - tokens_tensor_batch: (batch_size, max_sentece_length*2)\n",
    "            - segments_tensor_batch: (batch_size, max_sentece_length*2)\n",
    "            - labels_tensor_batch: (batch_size, max_sentece_length*2)\n",
    "        \"\"\"\n",
    "        # Get the number of training sentences\n",
    "        n_samples = tokens_tensor.size(0)\n",
    "        n_batches = n_samples // batch_size\n",
    "        n_samples = n_batches * batch_size\n",
    "\n",
    "        if not test:\n",
    "            # Get random sequence of samples\n",
    "            permutation = torch.randperm(n_samples)\n",
    "\n",
    "            tokens_tensor = tokens_tensor[permutation, :]\n",
    "            segments_tensor = segments_tensor[permutation, :]\n",
    "            labels = labels[permutation]\n",
    "\n",
    "            # Re-arrange into batches\n",
    "            tokens_batch = tokens_tensor.view(n_batches, batch_size,\n",
    "                                                        tokens_tensor.size(1))\n",
    "            segments_tensor_batch = segments_tensor.view(n_batches, batch_size,\n",
    "                                                        segments_tensor.size(1))\n",
    "            labels_tensor_batch = labels.view(n_batches, batch_size)\n",
    "\n",
    "            return tokens_batch, segments_tensor_batch, labels_tensor_batch\n",
    "        else:\n",
    "            order = torch.arange(0,n_samples).long()\n",
    "            tokens_tensor = tokens_tensor[order, :]\n",
    "            segments_tensor = segments_tensor[order, :]\n",
    "\n",
    "            # Re-arrange into batches\n",
    "            tokens_batch = tokens_tensor.view(n_batches, batch_size,\n",
    "                                                        tokens_tensor.size(1))\n",
    "            segments_tensor_batch = segments_tensor.view(n_batches, batch_size,\n",
    "                                                        segments_tensor.size(1))\n",
    "            return tokens_batch, segments_tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XsKrMSyHiUka"
   },
   "outputs": [],
   "source": [
    "# Get the input in the right format\n",
    "# Test prep corpus and the rest\n",
    "max_sent_length = 75\n",
    "train_tokens, train_segments = prep_corpus(english_train, zh_train, max_sent_length)\n",
    "train_scores = torch.tensor(scores_train)\n",
    "\n",
    "val_tokens, val_segments = prep_corpus(english_dev, zh_dev, max_sent_length)\n",
    "val_scores = torch.tensor(scores_dev)\n",
    "\n",
    "test_tokens, test_segments = prep_corpus(english_test, zh_test, max_sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "C1z-E3_qiJbt",
    "outputId": "aa1595d6-27d9-4fcd-bbfc-b05be83cd113"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pytorch_pretrained_bert.optimization:t_total value of -1 results in schedule not being applied\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training:\n",
      "[Epoch 1  ] ended with valid rmse:   0.91, pearson:  0.251\n",
      "[Epoch 2  ] ended with valid rmse:   0.90, pearson:  0.277\n",
      "[Epoch 3  ] ended with valid rmse:   0.89, pearson:  0.290\n",
      "[Epoch 4  ] ended with valid rmse:   0.88, pearson:  0.294\n",
      "[Epoch 5  ] ended with valid rmse:   0.88, pearson:  0.300\n",
      "[Epoch 6  ] ended with valid rmse:   0.88, pearson:  0.306\n",
      "[Epoch 7  ] ended with valid rmse:   0.88, pearson:  0.313\n",
      "[Epoch 8  ] ended with valid rmse:   0.88, pearson:  0.319\n",
      "[Epoch 9  ] ended with valid rmse:   0.88, pearson:  0.323\n",
      "[Epoch 10 ] ended with valid rmse:   0.88, pearson:  0.327\n",
      "[Epoch 11 ] ended with valid rmse:   0.88, pearson:  0.332\n",
      "[Epoch 12 ] ended with valid rmse:   0.88, pearson:  0.336\n",
      "[Epoch 13 ] ended with valid rmse:   0.88, pearson:  0.339\n",
      "[Epoch 14 ] ended with valid rmse:   0.87, pearson:  0.342\n",
      "[Epoch 15 ] ended with valid rmse:   0.87, pearson:  0.345\n",
      "[Epoch 16 ] ended with valid rmse:   0.87, pearson:  0.346\n",
      "[Epoch 17 ] ended with valid rmse:   0.88, pearson:  0.351\n",
      "[Epoch 18 ] ended with valid rmse:   0.87, pearson:  0.353\n",
      "[Epoch 19 ] ended with valid rmse:   0.88, pearson:  0.355\n",
      "[Epoch 20 ] ended with valid rmse:   0.87, pearson:  0.359\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = EvaluationModel()\n",
    "\n",
    "# Freeze bert layers\n",
    "for param in model.bert_layer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# Create the optimizer (no need for frozen parameters)\n",
    "model_optimizer = BertAdam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "print('Begin training:')\n",
    "# Train model:\n",
    "model.train_model(model_optimizer, train_tokens, train_segments,\n",
    "                  train_scores, val_tokens, val_segments, val_scores,\n",
    "                  test_tokens, test_segments, n_epochs=20, shuffle=False, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_regressors",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
